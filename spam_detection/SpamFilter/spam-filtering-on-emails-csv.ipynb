{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam filtering using LR, SVM, NN and Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/emails.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/kaggle/input/emails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [4 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "data['text'].replace({'Subject:': ''}, inplace=True, regex=True)\n",
    "X=data['text']\n",
    "y=data['spam']\n",
    "vectorizer=CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "nb=GaussianNB()\n",
    "svm=SVC(kernel='rbf')\n",
    "nn=NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n",
    "dt=tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training all 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/base.py:216: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,Y_train)\n",
    "# nb.fit(X_train,Y_train)\n",
    "svm.fit(X_train,Y_train)\n",
    "nn.fit(X_train,Y_train)\n",
    "dt.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predicted=lr.predict(X_test)\n",
    "svm_predicted=svm.predict(X_test)\n",
    "nn_predicted=nn.kneighbors(X_test)\n",
    "dt_predicted=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification arror for the 4 algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       885\n",
      "           1       0.98      0.97      0.97       261\n",
      "\n",
      "    accuracy                           0.99      1146\n",
      "   macro avg       0.98      0.98      0.98      1146\n",
      "weighted avg       0.99      0.99      0.99      1146\n",
      "\n",
      "SVM :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       885\n",
      "           1       1.00      0.10      0.19       261\n",
      "\n",
      "    accuracy                           0.80      1146\n",
      "   macro avg       0.90      0.55      0.54      1146\n",
      "weighted avg       0.84      0.80      0.72      1146\n",
      "\n",
      "Decision tree :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       885\n",
      "           1       0.89      0.88      0.88       261\n",
      "\n",
      "    accuracy                           0.95      1146\n",
      "   macro avg       0.93      0.92      0.93      1146\n",
      "weighted avg       0.95      0.95      0.95      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_cls_report=classification_report(Y_test,lr_predicted)\n",
    "svm_cls_report=classification_report(Y_test,svm_predicted)\n",
    "# nb_cls_report=classification_report(Y_test,nb_predicted)\n",
    "dt_cls_report=classification_report(Y_test,dt_predicted)\n",
    "print(\"Logistic regression : \"+lr_cls_report)\n",
    "print(\"SVM : \"+svm_cls_report)\n",
    "# print(\"Naive Bayes : \"+nb_cls_report)\n",
    "print(\"Decision tree : \"+dt_cls_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All 4 algos have done ok but LR has achieved the best performance. You'll NB algo is commenting because it needs more memory resources(run on aws sagemaker) as compared the algos shown here. So, although NB is known to be the best algo for spam filtering, as per the code run above, it requires more resources. Don't know why? If you know, please comment/modify the code and let me know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
